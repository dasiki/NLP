{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'blue'>**1、概率论**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 概率**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概率（probability) 是从随机试验中的事件到实数域的映射函数，用以表示事件发生的可能性。如果用 $P(A)$ 作为事件 $A$ 的概率， $\\Omega$ 是试验的样本空间，则概率必须满足如下三条公理：\n",
    "\n",
    "**公理1（非负性）** $P(A)\\geq 0$ \n",
    "\n",
    "**公理2（规范性）** $P(\\Omega) = 1$ \n",
    "\n",
    "**公理3（可列可加性）** 如果事件两两不相容，对于任意的 $i$ 和 $j(i\\neq j)$，事件 $A_i$ 和 $A_j$ 不相交（ $A_i\\cap A_j = \\phi$）则有，\n",
    "#### $$ P\\big(\\bigcup_{i=0}^{\\infty} A_i\\big) = \\sum_{i=0}^\\infty P(A_i)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 最大似然估计**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 $\\{s_1,s_2,\\cdots,s_n\\}$ 是一个试验的样本空间，在相同的情况下重复试验N次，观察到样本 $s_k(1\\leq k\\leq n)$ 的次数为 $n_N(s_k)$，那么， $s_k$ 在这 $N$ 次试验中的相对频率为 \n",
    "#### $$ q_N(s_k) = \\frac{n_N(s_k)}{N}$$ \n",
    "由于 $\\sum_{k=1}^n n_N(s_k) = N$，因此， $\\sum_{k=1}^n q_N(s_k) = 1$ 。当 $N$ 越来越大时，相对频率 $q_N(s_k)$ 就越来接近 $s_k$ 的概率 $P(s_k)$ 。即\n",
    "#### $$\\mathop{lim}\\limits_{N\\rightarrow \\infty} q_N(s_k) = P(s_k) $$ \n",
    "因此，通常用相对频率作为概率的估计值。这种估计概率值和方法称为最大似然估计（maximum likelihood estimation）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 条件概率**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 $A$ 和 $B$ 是样本空间 $\\Omega$ 上的两个事件。那么，在给定 $B$ 时 $A$ 的条件概率 $P(A|B)$ 为\n",
    "#### $$ P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$ \n",
    "条件概率 $P(A|B) $是给出了在已知事件 $B$ 发生的情况下，事件 $A$ 的概率。一般地， $P(A|B)\\neq P(A)$ \n",
    "\n",
    "条件概率的三个基本性质：\n",
    "\n",
    "（1）**非负性：** $P(A|B)\\geq 0$ \n",
    "\n",
    "（2）**规范性：** $P(\\Omega|B) = 1$ \n",
    "\n",
    "（3）**可列可加性：** 如果事件 $A_1,A_2,\\cdots$ 两两互不相容，则\n",
    "#### $$P\\big(\\sum_{i=1}^\\infty A_i|B\\big) = \\sum_{i=1}^\\infty P\\big(A_i|B\\big) $$ \n",
    "如果 $A_i,A_j$ 条件独立，当且仅当\n",
    "#### $$ P(A_i,A_j|B) = P(A_i|B)\\times P(A_j|B) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 贝叶斯法则**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯法则，又称贝叶斯理论，是条件概率的重要依据。根据条件概率的定义公式和乘法规则式，可得\n",
    "#### $$ P(B|A) = \\frac{P(B\\cap A)}{P(A)} = \\frac{P(A|B)P(B)}{P(A)}$$ \n",
    "上式中，分母 $P(A)$ 可以看作普通常量，因为我们只是关心给定事件 $A$ 的情况下可能发生事件 $B$ 的概率，故有\n",
    "\n",
    "#### $$ \\mathop{argmax}\\limits_B \\frac{P(A|B)P(B)}{P(A)} = \\mathop{argmax}\\limits_B P(A|B)P(B)$$ \n",
    "****\n",
    "假设 $B$ 是样本空间 $\\Omega$ 的一个划分，即 $\\sum_i B_i = \\Omega$。如果 $A\\subseteq \\bigcup_i B_i$，并且 $B_i$ 互不相交，那么 $A = \\sum_{i=1}B_iA$，于是 $P(A) = \\sum_{i=1}P(B_iA)$ 。由乘法定理可得：\n",
    "#### $$P(A) = \\sum_iP(A|B_i)P(B_i) $$ \n",
    "称为 **全概率公式** \n",
    "\n",
    "以下为贝叶斯法则的精确描述：\n",
    "\n",
    "假设 $A$ 为样本空间 $\\Omega$ 的事件， $B_1,B_2,\\cdots,B_n$ 为 $\\Omega$ 的一个划分，如果 $A\\subseteq \\bigcup_{i=1}^n B_i$， $P(A)>0$，并且 $i\\neq j, B_i\\cap B_j = \\phi$， $P(B_i) > 0 (i = 1,2,\\cdots,n)$，则\n",
    "#### $$P(B_j|A) = \\frac{P(A|B_j)P(B_j)}{P(A)} = \\frac{P(A|B_j)P(B_j)}{\\sum_i^nP(A|B_i)P(B_i)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.5 随机变量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机变量（random variable) 就是试验结果的函数。\n",
    "\n",
    "设 $X$ 是一离散型随机变量，其全部可能的值为 $\\{a_1,a_2,\\cdots\\}$。那么\n",
    "#### $$p_i = P(X = a_i),\\quad i=1,2,\\cdots $$ \n",
    "称为 $X$ 的概率函数。显然，$p_i\\geq 0,\\sum_{i=1}p_i = 1$。有时也称为随机变量 $X$ 的概率分布。\n",
    "\n",
    "此时，函数 $P(X\\leq x) = F(x) ,\\quad -\\infty < x < \\infty$ 称为 $X$ 的分布函数。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.6 二项分布**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果随机试验E满足： 将一个试验在相同条件下重复进行$n$次，各次试验仅有两个结果$A 和 \\mathop A\\limits^{-}$. 事件$A$ 的概率在各次试验中保持不变，$P(A) = p, P(\\mathop A\\limits^{-}) = 1-p$ ; 短吻间银鱼试验结果互不影响，则称随机试验E为n次伯努力试验。 如果$n$次伯努利试验，取得成功的次数为$X$的概率，可用以下公式描述：\n",
    "#### $$P = C(X,n)\\times p^x(1-p)^{(n-X)}$$\n",
    "**二项分布是最重要的离散型概率分布之一。** 在自然语言处理中，一般以句子为处理单位。为了简化问题的复杂性，通常假设一个句子的出现独立于它前面的其他句子，句子的概率分布近似地被认为符合二项式分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.7 联合概率分布和条件概率分布**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 $(X_1,X_2)$ 为一个二维的离散型随机变量， $X_1$ 全部可能的取值为 $a_1,a_2,\\cdots$；$X_2$ 全部可能取值为 $b_1,b_2,\\cdots$。那么，$(X_1,X_2)$ 的联合分布(joint distribution) 为\n",
    "#### $$ p_{ij} = P(X_1 = a_i, X_2 = b_j),\\quad i=1,2,\\cdots ; j=1,2,\\cdots$$ \n",
    "一个随机变量或向量 $X$ 的条件概率分布就是在某种给定条件之下 $X$ 的概率分布。\n",
    "#### $$ P(X_1 = a_i| X_2 = b_j) = \\frac{P(X_1 = a_i, X_2 = b_j)}{P(X_2 = b_j)} = \\frac{p_{ij}}{P(X_2 = b_j)}$$ \n",
    "由于 $P(X_2 = b_j) = \\sum_k p_{kj}$，故有\n",
    "#### $$ P(X_1 = a_i | X_2 = b_j) = \\frac{p_{ij}}{\\sum_k p_{ij}},\\quad i=1,2,\\cdots$$ \n",
    "类似地，\n",
    "#### $$P(X_2=b_j | X_1 = a_i) = \\frac{p_{ij}}{\\sum_k p_{ik}},\\quad j=1,2,\\cdots$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.8 贝叶斯决策理论**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设研究的分类问题有 $c$ 个类别，各类别的状态分别用 $\\omega_i$ 表示，$i=1,2,\\cdots,c$；对应于各个类别 $\\omega_i$ 出现的先验概率为 $P(\\omega_i)$；在特征空间已经观察到某一向量  $x$， $x = [x_1,x_2,\\cdots,x_n]^T$ 是 $d$ 维特征空间上的某一点，且条件概率密度函数 $p(x|w_i)$ 是已知的。那么，利用贝叶斯公式，我们可以得到后验概率 $P(w_i|x)$ 如下：\n",
    "#### $$ P(w_i|x) = \\frac{p(x|w_i)p(w_i)}{\\sum_{j=1}^c P(x|w_j)P(w_j)}$$ \n",
    "基于最小错误率的贝叶斯决策规则为：\n",
    "\n",
    "（1）如果 $P(w_i|x) = \\mathop{max}\\limits_{j=1,2,\\cdots,c} P(w_j|x)$，那么， $x\\in w_i$ ；或者说，\n",
    "\n",
    "（2）如果$P(x|w_i)P(w_i) = \\mathop{max}\\limits_{j=1,2,\\cdots,c} P(x|w_j)P(w_j)$，则 $x\\in w_i$；\n",
    "\n",
    "如果类别只有2类，即 $c=2$，则有：\n",
    "\n",
    "（3）如果 $l(x) = \\frac{P(x|w_1)}{P(x|w_2)}>\\frac{P(w_2)}{P(w_1)}$，则 $x\\in w_1$，否则 $x\\in w_2$ ；其中，$l(x)$ 为似然比(likehood ratio)，而 $\\frac{P(w_2)}{P(w_1)}$ 称为似然比阈值（threshold） \n",
    "\n",
    "<font size=4 color='red'><b>贝叶斯决策理论在自然语言处理中的词义消歧（word sense disambiguation，WSD）、文本分类等问题的研究中具有重要用途。</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.9 期望和方差**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期望（exceptation）是指随机变量所取值的概率平均。假设 $X$ 为一随机变量，其概率分布为 $P(X=x_k) = p_k,k=1,2,\\cdots$，若级数 $\\sum_{k=1}^\\infty x_kp_k$ 绝对收敛，那么，随机变量 $X$ 的数学期望或概率平均值为\n",
    "#### $$ E(X) = \\sum_{k=1}^\\infty x_kp_k$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'blue'>**2、信息论**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 熵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 $X$ 是一个离散型随机变量，取值空间为 $\\mathcal{R}$，其概率分布为 $p(x) = P(X=x),x\\in \\mathcal{R}$。那么， $X$ 的熵 $H(X)$ 定义为\n",
    "#### $$ H(X) = -\\sum_{x\\in R}p(x)log_2p(x)$$ \n",
    "其中，约定 $0log0=0$。 $H(X)$ 可以写为 $H(p)$。\n",
    "\n",
    "**熵又称为自信息（self-information），可以视为描述一个随机变量的不确定性的数量。它表示信源 $X$ 每发一个信号（不论发什么符号）所提供的平均信息量。一个随机变量的熵越大，它的不确定性越大。那么，正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 联合熵和条件熵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 $X,Y$是一对离散型随机变量 $X,Y\\sim p(x,y)$，$X,Y$的联合熵（joint entropy） $H(X,Y)$ 定义为：\n",
    "#### $$ H(X,Y) = -\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)logp(x,y)$$\n",
    "联合熵实际上就是描述一对随机变量平均所需要的信息量。\n",
    "\n",
    "给定随机变量 $X$ 的情况下，随机变量 $Y$ 的条件熵（conditional）定义为：\n",
    "#### $$ \\begin{align} H(Y|X) &= \\sum_{x\\in X}p(x)H(Y|X=x) \\\\\n",
    "                             &= \\sum_{x\\in X}p(x)[-\\sum_{y\\in Y}p(y|x)logp(y|x)] \\\\\n",
    "                             &= -\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)logp(y|x) \\end{align}$$ \n",
    "\n",
    "根据上式，可得\n",
    "#### $$ \\begin{align} H(X,Y) &= -\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)logp(x,y) \\\\\n",
    "                             &= -\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)log[p(x)p(y|x)] \\\\\n",
    "                             &= -\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)[logp(x) + logp(y|x)] \\\\ \n",
    "                             &= -\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)logp(x)- \\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)logp(y|x) \\\\\n",
    "                             &= -\\sum_{x\\in X}p(x)logp(x)- \\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)logp(y|x) \\\\\n",
    "                             &= H(x) + H(Y|X)\\end{align}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 互信息**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据熵的连锁规则，有\n",
    "#### $$ H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$$ \n",
    "因此，\n",
    "#### $$H(X) -H(X|Y)  = H(Y) - H(Y|X)  $$\n",
    "这个差叫做 $X$ 和 $Y$的互信息（mutual information,MI）,记作 $I(X;Y)$。\n",
    "\n",
    "或定义为：如果 $(X,Y) \\sim p(x,y)$，则 $X,Y$ 之间的互信息 $I(X;Y) = H(X) - H(X|Y)$ \n",
    "\n",
    "**$I(X;Y)$ 反映了是在知道了 $Y$ 的值以后 $X$ 的不确定性的减少量。可以理解为 $Y$ 的值透露了多少关于 $X$ 的信息量。**\n",
    "\n",
    "**互信息体现了两变量之间的依赖关系：如果 $I(X;Y)>>0$ ，表明 $X$ 和 $Y$ 是高度相关的；如果 $I(X;Y)=0$，表示 $X$ 和 $Y$ 是相互独立的； 如果$I(X;Y)<<0$，表示 $Y$ 的出现不但未使 $X$ 的不确定性减少，反而增大了 $X$的不确定性，是不利的。**\n",
    "\n",
    "<img src=\"nlp_互信息.png\" width = 400></img>\n",
    "\n",
    "<font size=4 color='red'><b>互信息在词汇聚类（word clustering）、汉语自动分词、词义消歧等问题的研究中具有重要用途。</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 相对熵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对熵(relative entropy) 又称为 Kullback-Leibler差异（Kullback-Leibler divergence），或简称为KL距离，是衡量相同事件空间中两个概率分布相对差距的测度。两个概率分布 $p(x)$ 和 $q(x)$ 的相对熵定义为\n",
    "#### $$ D(p||q) = \\sum_{x\\in X} log \\frac{p(x)}{q(x)}$$\n",
    "其中，约定$0log(0/q) = \\infty$ 。表示成期望值为\n",
    "#### $$ D(p||q) = E_p\\big( log \\frac{p(X)}{q(X)}\\big)$$ \n",
    "当两个随机分布完全相同时，即$p=q$，其相对熵为0。当两个随机分布的差别增加时，其相对熵期望值也增大。\n",
    "\n",
    "互信息实际上就是衡量一个联合分布与独立性差距多大的测度：\n",
    "#### $$ I(X;Y) = D(p(x,y)||p(x)p(y))$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.5 交叉熵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵的概念是用来衡量估计模型与真实概率分布之间差异情况的。\n",
    "\n",
    "如果一个随机变量 $X \\sim p(x),q(x)$ 为用于近似 $p(x)$的概率分布，那么，随机变量 $X$ 和模型 $q$ 之间的交叉熵(cross entropy) 定义为：\n",
    "#### $$ H(X,q) = H(X) + D(p||q) = -\\sum_xp(x)logq(x) = E_p\\big(log\\frac{1}{q(x)}\\big)$$ \n",
    "由此，可以定义语言 $L = (X_i)\\sim p(x)$ 与其模型 $q$ 的交叉熵为\n",
    "#### $$ H(L,q) = -\\mathop{lim}\\limits_{n\\rightarrow \\infty} \\frac{1}{n}\\sum_{x_1^n}p(x_1^n)log q(x_1^n)$$ \n",
    "其中，$x_1^n = x_1,x_2,\\cdots,x_n$ 为 $L$ 的语句， $p(x_1^n)$ 为 $L$ 中 $x_1^n$ 的概率， $q(x_1^n)$ 为模型 $q$ 对 $x_1^n$ 的概率预估。 \n",
    "\n",
    "根据信息论的定理：假定语言 $L$ 是稳定(stationary)遍历的(ergodic)随机过程， $L$ 与其模型 $q$ 的交叉熵计算公式变为 \n",
    "#### $$ H(L,q) = -\\frac{1}{n}log q(x_1^n)$$ \n",
    "\n",
    "**交叉熵与模型在测试语料中分配给每个单词的平均概率所表达的含义正好相反，模型的交叉熵越小，模型的表现越好。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6 困惑度**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在设计语言模型时，我们通常用困惑度（perplexity）来代替交叉熵衡量语言模型的好坏。给定语言 $L$ 的样本 $l_1^n = l_1\\cdotsl_n,L$ 的困惑度 $PP_q$ 定义为\n",
    "### $$ PP_q = 2^{H(L,q)} = 2^{-\\frac{1}{n}logq(l_1^n)} = [q(l_1^n)]^{-\\frac{1}{n}}$$ \n",
    "\n",
    "**语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。**\n",
    "\n",
    "在自然语言处理中，我们所说的语言模型的困惑度通常是指语言模型对于测试数据的困惑度。一般情况下，将所有数据分成两个部分，一部分作为训练数据，用于估计模型的参数；另一部分作为测试数据，用于评估语言模型的质量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.7 噪声信道模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'blue'>**3、支持向量机**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 线性分类**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 线性不可分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3 构造核函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'nlp2.png' width = 600></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
